---

#available choices
env_names: ['cartpole_custom-v1', 'halfcheetah_custom-v1', 'halfcheetah_custom_norm-v1', 'halfcheetah_custom_rand-v1', 'halfcheetah_custom_rand-v2', 'lunarlander_custom_default_rand-v0','hopper_custom_rand-v1']
types_particles: ["discrete","continuous"] #???: which one is better?


#values
debug_mode:

    #General
    tr_eps: 7
    verbose: 1 #or: False/True (False/0: display progress bar; True/1: display 1 log newline per episode) 
    
    #Seed
    seeds: [1,2]
    # seed: seeds[1]
    
    #Env
    env_name: 'hopper_custom_rand-v2' #env_names[-2]
    n_workers: &n_workers_debug 3 #n_envs
    
    #MAML
    # n: 2 #no. of NN layers
    h_maml: 64 #size of hidden layers
    lr_maml: 0.5 #adaptation step size / inner learning rate
    # beta:0.001 #meta step size / outer learning rate #TIP: controlled and adapted by TRPO (in maml step) [to guarantee monotonic improvement, etc]
    gamma_maml: 0.99
    b_maml: *n_workers_debug #batch size: Number of trajectories (rollouts) to sample from each task
    #TRPO
    max_grad_kl: 1.0e-2
    max_backtracks: 15
    accept_ratio: 0.1
    zeta: 0.8 #0.5
    rdotr_tol: 1.0e-10
    nsteps: 10
    damping: 1.0e-5 #0.01
    
    #DDPG
    lr_ddpg: 0.01 #learning rate
    h1_ddpg: 64
    h2_ddpg: 64
    gamma_ddpg: 0.99 #discount factor
    T_ddpg_init: 10 #number of timesteps before any updates
    b_ddpg: 100 #batch size
    
    #Discriminator
    r_disc_scale: 1.0 #reward scale
    h_disc: 32
    lr_disc: 0.02 
    b_disc: 128
    
    #SVPG
    n_particles: *n_workers_debug
    temp: 10.0 #temperature
    type_particles: "discrete" #types_particles[0]
    kld_coeff: 0.0 #kld : KL Divergence
    T_svpg_reset: 25 #how often to fully reset svpg particles
    delta_max: 0.05 #maximum allowable change to env randomization params caused by svpg particles (If discrete, this is fixed, If continuous, this is max)
    T_svpg_init: 1 #number of svpg steps to take before updates
    T_svpg: &T_svpg_debug 2 #length of one svpg particle rollout
    lr_svpg: 0.03
    gamma_svpg: 0.99
    h_svpg: 16
    T_rand_rollout: *T_svpg_debug #for UDR
    
    #Evaluation
    evaluate: True
    log_ival: 1
    eval_eps: 2

#============================================
#============================================

run_mode:

    #General
    tr_eps: 250 #500
    verbose: 1 #or: False/True (False/0: display progress bar; True/1: display 1 log newline per episode) 
    
    #Seed
    # seeds: [None,1,2,3,4,5]
    seeds: [1,2,4]
    
    #Env
    env_name: 'hopper_custom_rand-v2' #env_names[-2]
    n_workers: &n_workers_run 10 #n_particles #mp.cpu_count() - 1 #:n_envs
    
    #MAML
    # n: 2 #no. of NN layers
    h_maml: 256 #100 #size of hidden layers
    lr_maml: &lr_agent 0.001 #adaptation step size / inner learning rate
    gamma_maml: 0.99
    b_maml: *n_workers_run #batch size: Number of trajectories (rollouts) to sample from each task
    #TRPO
    max_grad_kl: 1.0e-2 #*lr_agent / 10. #beta=0.001 (meta step size / outer learning rate)
    max_backtracks: 15
    accept_ratio: 0.1
    zeta: 0.8 #0.5
    rdotr_tol: 1.0e-10
    nsteps: 10
    damping: 1.0e-5 #0.01
    
    #DDPG
    lr_ddpg: *lr_agent #learning rate
    h1_ddpg: 100 #400
    h2_ddpg: 100 #300
    gamma_ddpg: 0.99 #discount factor
    T_ddpg_init: 1000 #number of timesteps before any updates
    b_ddpg: 1000 #100 #batch size
    
    #Discriminator
    r_disc_scale: 1.0 #reward scale
    h_disc: 128
    lr_disc: 0.002 
    b_disc: 128
    
    #SVPG
    n_particles: *n_workers_run
    temp: 10.0 #temperature
    type_particles: "discrete" #types_particles[0]
    kld_coeff: 0.0 #kld : KL Divergence
    T_svpg_reset: 25 #how often to fully reset svpg particles
    delta_max: 0.05 #maximum allowable change to env randomization params caused by svpg particles (If discrete, this is fixed, If continuous, this is max)
    T_svpg_init: 50 #100 #1000 #number of svpg steps to take before updates
    T_svpg: &T_svpg_run 5 #length of one svpg particle rollout
    lr_svpg: 0.0003
    gamma_svpg: 0.99
    h_svpg: 100
    T_rand_rollout: *T_svpg_run #for UDR
    
    #Evaluation
    evaluate: True
    log_ival: 1
    eval_eps: 3
    
...
